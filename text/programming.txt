Bjarne Stroustrup (1950 - )

David Wheeler (1927-2004):
    "All problems in computer science cab be solved by another level of indirection
     [, except for the problem of too many layers of indirection."
     
     "Compatibility means deliberately repeating other people's mistakes."

The minimal C++ program is
    int main() {}

{} - curly braces express grouping in C++.
// - double slash

main() - global function name.

2.2.2 Types, Variables, and Arithmetic

Every name and every expression has a type that determines the operations that
may be performed on it.

Declaration is a statement that introduces a name into the program. It specifies
a type for the name entity:
    - a type: set of possible values and a set of operations;
    - an object is some memory that holds a value of some type;
    - a value is a set of bits interpreted according to type;
    - a variable is a named object.

C++ fundamental types:
    - bool;
    - char;         - character (typically an 8 bit), sizeof(char) = 1.
    - int;
    - double;

    x+y,        plus
    +x,         unary plus
    x-y,        minus
    -x,         unary minus
    x*y,        multiply
    x/y,        divide
    x%y         remainder (modulus) for integers
    
    =           assignment
    
    x==y,       equal
    x!=y,       not equal
    x<y,        less than
    x>y,        greater than
    x<=y,       less than or equal
    x>=y,       greater than or equal

2.2.3 Constants
    const       
    constexpr

3. A tour of C++: Abstraction Mechanisms

User defined types:
    - concrete classes;
    - abstract classes;
    - class hierarchies.      => object oriented programming

Template is a mechanism for parameterizing types and algorithms. => generic programming.

3.2 Classes
-----------
The central language feature of C++ is the class.
A class is a user defined type provided to represent a concept in the code of a program.

3.2.1 Concrete types

just like build-in types

A constructor without an argument is called a default constructor.

A container is an object holding a collection of elements.
Initializer list contructor.

3.2.2 Abstract types

is a type that completely insulates a user from implementation details.
is a pure interface to specific defined later.
virtual = may be redefined later in a class derived from this one.

:public = is derived from  or is a subtype of

3.3 Copy and Move
-----------------

By default objects can be copied.

&& = rvalue reference is a reference to which we can bind an rvalue.

4. A tour of C++: Containers and Algorithms
-------------------------------------------

string, ostream, vector, map, unique_ptr, thread, regex, complex

Standard library:
    - run time language support (allocation, type information);
    - C standard library
    - string and I/O streams
    - framework of containers and algorithms
    - numerical computation
    - regex
    - concurrent programming - threads, locks
    - templates - pair, clock
    - smart pointers;



What every programmer should know about memory
==============================================

As CPU cores becomes both faster and more numerous, the limiting factor for most
programs is now, and will be for some time, memory access.
Hardware designers have come up with ever more sophisticated memory handling and
acceleration techniques - such as CPU caches - but these cannot work optimally
without some help from the programmer. Unfortunately, neither the structure nor
the cost of using the memory subsystem of a computer or the caches on CPUs is
well understood by most programmers. This paper explains the structure of memory
subsystems in use on modern commodity hardware, illustrating why CPU caches were
developed, how they work, and what programs should do to achieve optimal
performance by utilizing them.

1. Introduction
---------------
In the early days computers were much simpler.
The various components of a system, such as the CPU, memory, mass storage, and
network interfaces, were developed together and, as result, were quite balanced
in their performances. For examples, the memory and network interfaces were
not much faster than the CPU at providing data.

This situation changed once the basic structure of computers stabilized and
hardware developers concentrated on optimizing individual subsystems.
Suddenly the performance of some components of the computer fell significantly
true for mass storage and memory subsystems which, for cost reasons, improved
more slowly relative to other components.

The slowness of mass storage has mostly been dealt with using software techniques:
    -   operation systems keep most often used data in main memory, which can be
        accessed at a rate orders of magnitude faster than the hard disk.
    -   Cache storage was added to the storage devices themselves, which requires
        no changes in the operation system to increase performance.

Unlike storage subsystems, removing the main memory as a bottleneck has proven
much more difficult and almost all solutions require changes to the hardware.
Today these changes mainly come in the following forms:
    -   RAM hardware design;
    -   Memory controller designs.
    -   CPU caches
    -   DMA (Direct memory access) for devices.


This document is mostly for software developers. It does not go into enough
technical details of the hardware to be useful for hardware oriented readers.


2. Commodity Hardware Today
---------------------------

Over the years personal computers and smaller servers standardized on a chipset
with two parts:
    -   the Northbridge
    -   the Southbridge

            +-------+   +-------+
            | CPU 1 |   | CPU 2 |
            +-------+   +-------+
                |    FSB    |
                +-----------+
                |           |
            +-------------------+
            |     Northbridge   | --- RAM
            +-------------------+
                      |
            +-------------------+
            |     Southbridge   | --- PCI-E, SATA, USB
            +-------------------+

All CPUs are connected via a common bus (FSB = Frond Side Bus) to Northbridge.
The Northbridge contains, among other things, the memory controller, and its
implementation determines the type of RAM chips used for the computer.
Different types of RAM, such as DRAM, Rambus, and SDRAM, reqire different memory
controllers.

Southbridge = I/O bridge, handles communication with devices through a variety
of different buses.

    -   All data communication from one CPU to another must travel over the same
        bus used to communicate with the Northbridge.
    -   All communication with RAM must pass through the Northbridge.
    -   The RAM has only a single port.
    -   Communication between a CPU and a device attached to the Southbridge is
        routed through the Northbridge.

In the earliest days of the PC, all communication with devices on either bridge
had to pass through the CPU, negatively impacting overall system performance.
To work around this problem some devices became capable of direct memory access.
DMA allows devices, with the help of the Northbridge, to store and receive data
in RAM directly without the intervention of the CPU (and its inherent performance
cost). Today all high performance devices attached to any of the buses can
utilize DMA. While this greatly reduces the workload on the CPU, it also creates
contention for the bandwidth of the Northbridge as DMA requests complete with
RAM access from the CPUs. This problem, therefore, must be taken into account.

A second bottleneck involves the bus from the Northbridge to the RAM. The exact
details of the bus depend on the memory types deployed. On older systems there
is only one bus to all the RAM chips, so parallel access is not possible.
Recent RAM types require two separate buses (or channels as they are called for
DDR2) which doubles the available bandwidth. The Northbridge interleaves memory
access the channels.

It's important for performance to schedule memory access in ways that minimize
delays. Processors are much faster and must wait to access memory, despite the
use of CPU caches. If multiple hyper threads, cores, or processors access memory
at the same time, the wait times for memory access are even longer. This is also
true for DMA operations.

Using multiple external memory controllers is not the only way to increase memory
bandwidth. The other increasingly popular way is to integrate memory controllers
into the CPUs and attach memory to each CPU.

NUMA = Non Uniform Memory Architecture

Local memory attached to a processor can be accessed with usual speed. The
situation is different when memory attached to another processor is accessed.
In this case the interconnects between the processors have to be used.
Each such communication has an associated cost. We talk about NUMA factors.


2.1 RAM Types
-------------

    -   Static  (SRAM)
    -   Dynamic (DRAM)

SRAM is much more expensive to produce.

SRAM >= 6 transistors. The state is stable as long as power is available.

Dynamic RAM is much simpler than static RAM.
It keeps its state in the Capacity C.
Transistor M is used to guard the access to the state.
C ~= femto farad or lower
Resistance is tera ohms
It only takes a short time for the capacity to dissipate.
This problem is called leakage. And need to constantly refreshed every 64 ms.
During the refresh no access to the memory is possible since a refresh is simply
a memory read operation where the result is discarded.
A second problem resulting from the tiny charge is that the information read from
the cell is not directly usable.
A third problem is that reading a cell causes the charge of the capacitor to be
depleted. This means every read operation must be followed by an operation to
recharge the capacitor. This is done automatically by feeding the output of the
sense amplifier back into the capacitor. It does mean, though, the reading
memory content requires additional energy and, more importantly, time.
A fourth problem is that charging and draining a capacitor is not instantaneous.
This means it takes some time for the capacitor to be charged and discharged.

We have to live with main memory which is based on DRAM.

A program selects a memory location using a virtual address.
The processor translates this into a physical address and finally the memory
controller selects the RAM chip corresponding to that address.
It would be completely impractical to address memory locations individually from
the memory controller- 4GB of RAM would require 2^32 address lines.
Instead the address is passed encoded as a binary number using a smaller set
of address lines. The address passed to the DRAM chip this way must be
demultiplexed first. A demultiplexer with N address lines will have 2^N output
lines. These output lines can be used to select the memory cell.
Using this direct approach is no big problem for chip with small capacities.
But if the number of cells grows this approach is not suitable anymore
A chip with 1 Gbit  capacity would need 30 address lines and 2^30 select lines.
A demultiplexer for 30 address line needs a whole lot of chip real estate in
addition to the complexity of the demultiplexer.
The DRAM cells are organized in rows and columns.
With the array approach the design can get by with one demultiplexer and one
multiplexer of half the size.

SDRAM = Synchronous DRAM - clock from FSB:
    -   800  MHz
    -   1066 MHz
    -   1333 MHz
    -   1600 MHz

CLK
RAS strobe - Row Address
CAS strobe - Col Address
Data

With all this preparation to get the data it would be wasteful to only transfer
one data word.

SDR = Single Data Rate
DDR = Double Data Rate

Hardware and software prefetching can be used to create more overlap in the
timing and reduce the stall.

3. CPU caches
=============
CPU are today much more sophisticated than they were only 25 years ago.
The size of the SRAM used for caches is many times smaller than the main memory.

Main memory
    -   L3 cache
    -   L2 cache
        -   L1d cache
        -   L1i cache

In addition we have processors which have multiple cores and each core can have
multiple threads. The difference between a core and a thread is that separate
cores have separate copies of almost all the hardware resources.

By default all data read or written by the CPU cores is stored in the cache.
There are many regions which cannot be cached but this is something only the
OS implementers have to be concerned about; it not visible to the application
programmer. There are also instructions which allow the programmer to deliberately
bypass certain caches.

If the CPU needs a data word the caches are searched first.
Obviously, the caches cannot contain the content of the entire main memory, but since all
memory addresses are cacheable, each cache entry is tagged using the address of the data
word in the main memory. This way a request to read or write to an address can search
the caches for a matching tag. The address in this context can be either the virtual
or physical address, varying based on the caches implementation.

Since for the tag, in addition to the actual memory, additional space is required, it is
inefficient to choose a word as the granularity of the cache. For a 32 bit word on an x86
machine the tag itself might need 32 bits or more. Furthermore, since spatial locality
is one of the principles on which caches are based, it would be bad to not take this into
account. Since neighboring memory is likely to be used together it should also be loaded into
cache together. Remember also what we learned - RAM modules are much more effective if they
can transport many data words in a row. So the entries stored in the caches are not single
words but, instead, lines of several contiguous words. In early caches these lines were 32 bytes
long; nowadays the norm is 64 bytes.

When memory content is needs by the processor the entire cache line is load into the L1d.
The memory address for each cache line is computed by masking the address value according to
the cache line size. For a 64 bytes cache line this means the low 6 bits are zeroed.
The discarded bits are used as the offset into the cache line. The remaining bits are in some
cases used to locate the line in the cache and as the tag.
In practice an address value is split into three parts.

T - tag
S - Cache set
O - offset

When an instruction modifies memory the processor still has to load a cache line
first because no instruction modifies an entire cache line at once.
The content of the cache line before the write operation therefore has to be
loaded. It is not possible for a cache to hold partial cache lines.
A cache line which has been written to and which has not been written back to
main memory is said to be dirty. Once it is written the dirty flag is cleared.

To be able to load new data in a cache it is almost always first necessary to make
room in the cache. An eviction from L1d pushes the cache line down into L2 (which uses
the same cache line size). This of course means room has to be made in L2. This in
turn might push the content L3 and ultimately into main memory. Each eviction is
progressively more expensive. What is described here is the model for an exclusive
cache as is preferred by modern AMD and VIA processors. Intel implements inclusive
caches where each line in L1d is also present in L2. Therefore evicting


